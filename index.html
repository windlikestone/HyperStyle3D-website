<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>HyperStyle3D</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/3Dimage.png"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">HyperStyle3D: Text-Guided 3D Portrait Stylization via Hypernetworks</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Zhuo Chen<sup>1</sup>,</span>
            <span class="author-block">
              Xudong Xu<sup>2</sup>,</span>
            <span class="author-block">
              Yicaho Yan<sup>1</sup>,
            </span>
            <span class="author-block">
              Zhengqin Xu<sup>1</sup>,
            </span>
            <span class="author-block">
              Ye Pan<sup>1</sup>,
            </span>
            <span class="author-block">
              Wenhan Zhu<sup>1</sup>,
            </span>
            <span class="author-block">
              Wayne Wu<sup>2</sup>
            </span>
            <span class="author-block">
              Bo Dai<sup>2</sup>,
            </span>
            <span class="author-block">
              Xiaokang Yang<sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>MoE Key Lab of Artificial Intelligence, Shanghai Jiao Tong University,</span>
            <span class="author-block"><sup>2</sup>Shanghai AI Laboratory</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="./static/pdf/HyperStyle3D_Text-Guided_3D_Portrait_Stylization_via_Hypernetworks.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2304.09463"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/zichongc/HyperStyle3D-pytorch"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Unofficial Code</span>
                  </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/windlikestone/HyperStyle3D"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Official Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://g-1nonly.github.io/EvaSurf-Website/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data (Coming Soon)</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<div class="my-hr">
  <hr>
</div>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <div class="columns is-centered has-text-centered">
          <div class="column content">

            <!-- <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
              <source src="./static/video/introcution_video.mp4"
                    type="video/mp4">
            </video> -->

            <!-- <img src="./static/images/real_dataset.png" alt="Pulpit rock" width="1598" height="678"> -->
            
          </div>
        </div>
      </div>

    </div>
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- <p>
            We present the first method capable of photorealistically reconstructing a non-rigidly
            deforming scene using photos/videos captured casually from mobile phones.
          </p> -->
          <p>
            Portrait stylization is a long-standing task enabling extensive applications. Although 2D-based methods have made great progress in recent years, real-world applications such as metaverse and games often demand 3D content. On the other hand, the requirement of 3D data, which is costly to acquire, significantly impedes the development of 3D portrait stylization methods. In this paper, inspired by the success of 3D-aware GANs that bridge 2D and 3D domains with 3D fields as the intermediate representation for rendering 2D images, we propose a novel method, dubbed HyperStyle3D, based on 3D-aware GANs for 3D portrait stylization. At the core of our method is a hyper-network learned to manipulate the parameters of the generator in a single forward pass. It not only offers a strong capacity to handle multiple styles with a single model, but also enables flexible fine-grained stylization that affects only texture, shape, or local part of the portrait. While the use of 3D-aware GANs bypasses the requirement of 3D data, we further alleviate the necessity of style images with the CLIP model being the stylization guidance. We conduct an extensive set of experiments across the style, attribute, and shape, and meanwhile, measure the 3D consistency. These experiments demonstrate the superior capability of our HyperStyle3D model in rendering 3D-consistent images in diverse styles, deforming the face shape, and editing various attributes.
          </p>
          <!-- p>
            We show that <span class="dnerf">Nerfies</span> can turn casually captured selfie
            photos/videos into deformable NeRF
            models that allow for photorealistic renderings of the subject from arbitrary
            viewpoints, which we dub <i>"nerfies"</i>. We evaluate our method by collecting data
            using a
            rig with two mobile phones that take time-synchronized photos, yielding train/validation
            images of the same pose at different viewpoints. We show that our method faithfully
            reconstructs non-rigidly deforming scenes and reproduces unseen views with high
            fidelity.
          </p> -->
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
              <source src="./static/video/introduction_video.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <h2 class="title is-3">Method Overview</h2>
      <p></p>
    </div>

    <div class="columns is-centered has-text-justified">
      <div class="column">
            <p>
            The overview of our full-pipeline. (a) Our hyper-module consists of three trainable hyper-networks and a fixed text encoder. The text prompts of three levels (shape, attribute, and style) are encoded into the coarse, medium, and fine direction features, which are then fed into the corresponding hyper-network. The hyper-networks predict three groups of parameter offsets for the coarse, medium, and fine layers in the pre-trained 3D-aware generator. (b) Our hyper-module is trained under the supervision of CLIP loss and ID loss. Text prompts of three levels are simultaneously integrated into the training, empowering the hyper-module to handle the overlying manipulation of diverse styles, attributes, and shapes. We pre-define the source text as a description related to the current training target text, such as “Face” to “Bearded face”
            </p>
            <!-- <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
              <source src="./static/video/method.mp4"
                    type="video/mp4">
            </video> -->
             <br>
            <img src="./static/images/inference_pipeline_final.png" alt="Pulpit rock" width="2050" height="534">
            <img src="./static/images/training_pipeline_final.png" alt="Pulpit rock" width="2050" height="534">
            

      </div>
    </div>

    
    </div>
    
</section>

<!-- <section class="section">
  <div class="container is-max-desktop">
    
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Results on Real-world Dataset</h2>
        <div class="content has-text-justified">
          <p>
          </p>
        </div>

        <table>
          <tr>
            <td><img src="./static/images/tiger_ori.gif"></td>
            <td><img src="./static/images/tiger.gif"></td>
            <td><img src="./static/images/shoe.gif"></td>
            <td><img src="./static/images/golden_shoe.gif"></td>
          </tr>
          
          <tr>
            <td colspan="2" align="center"><span style="font-size:18px;"><em><strong>"A vegetable toy tiger"</strong></em></span></td>
            <td colspan="2" align="center"><span style="font-size:18px;"><em><strong>"A golden sneaker"</strong></em></span></td>
          </tr>
          <tr>
            <td><img src="./static/images/swiss_ori.gif"></td>
            <td><img src="./static/images/swiss.gif"></td>
            <td><img src="./static/images/nike_ori.gif"></td>
            <td><img src="./static/images/nike.gif"></td>
          </tr>
          <tr>
            <td colspan="2" align="center"><span style="font-size:18px;"><em><strong>"A Swiss bag"</strong></em></span></td>
            <td colspan="2" align="center"><span style="font-size:18px;"><em><strong>"A Nike sneaker"</strong></em></span></td>
          </tr>

          <tr>
            <td><img src="./static/images/tiffany_ori.gif"></td>
            <td><img src="./static/images/tiffany.gif"></td>
            <td><img src="./static/images/pineapple_ori.gif"></td>
            <td><img src="./static/images/pineapple.gif"></td>
          </tr>
          <tr>
            <td colspan="2" align="center"><span style="font-size:18px;"><em><strong>"A Tiffany blue bag"</strong></em></span></td>
            <td colspan="2" align="center"><span style="font-size:18px;"><em><strong>"A pineapple-like hat"</strong></em></span></td>
          </tr>

          <tr>
            <td><img src="./static/images/star_ori.gif"></td>
            <td><img src="./static/images/star.gif"></td>
            <td><img src="./static/images/red_bull_ori.gif"></td>
            <td><img src="./static/images/red_bull.gif"></td>
          </tr>
          <tr>
            <td colspan="2" align="center"><span style="font-size:18px;"><em><strong>"A cap with stars on it"</strong></em></span></td>
            <td colspan="2" align="center"><span style="font-size:18px;"><em><strong>"A Red Bull energy drink"</strong></em></span></td>
          </tr>

           <tr>
            <td><img src="./static/images/duck_ori.gif"></td>
            <td><img src="./static/images/duck.gif"></td>
            <td><img src="./static/images/pig_ori.gif"></td>
            <td><img src="./static/images/pig.gif"></td>
          </tr>
          <tr>
            <td colspan="2" align="center"><span style="font-size:18px;"><em><strong>"A Mallard"</strong></em></span></td>
            <td colspan="2" align="center"><span style="font-size:18px;"><em><strong>"A pink porcelain piggy toy"</strong></em></span></td>
          </tr>
        </table>
        </div>
      </div>


  </div> -->


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{chen2023hyperstyle3d,
  title={HyperStyle3D: Text-Guided 3D Portrait Stylization via Hypernetworks},
  author={Chen, Zhuo and Xu, Xudong and Yan, Yichao and Pan, Ye and Zhu, Wenhan and Wu, Wayne and Dai, Bo and Yang, Xiaokang},
  journal={arXiv preprint arXiv:2304.09463},
  year={2023}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is constructed using the source code provided by <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
            We are appreciate for their template.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
